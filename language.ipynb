{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to datatset: [open here](https://bark.phon.ioc.ee/voxlingua107/)\n",
    "\n",
    "Python 3.12.7 recommended\n",
    "\n",
    "Folder structure:\n",
    "```\n",
    "├── data\n",
    "│   ├── en\n",
    "│   │   ├── en_1.wav\n",
    "│   │   ├── en_2.wav\n",
    "│   │   ├── ...\n",
    "│   ├── fr\n",
    "│   │   ├── fr_1.wav\n",
    "│   │   ├── fr_2.wav\n",
    "│   │   ├── ...\n",
    "│   ├── ...\n",
    "language.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNLOAD VOXLINGUA107 DATASET WITH THIS SCRIPT\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Base URL for VoxLingua107 zip files\n",
    "base_url = \"https://bark.phon.ioc.ee/voxlingua107/\"\n",
    "\n",
    "# Directory to store the downloaded data\n",
    "data_dir = \"data\"\n",
    "\n",
    "# Function to download and extract a language zip file\n",
    "def download_and_extract(url):\n",
    "    lang_code = url.split(\"/\")[-1].split(\".\")[0]\n",
    "    lang_dir = os.path.join(data_dir, lang_code)\n",
    "\n",
    "    # Create directory for the language if it doesn't exist\n",
    "    if not os.path.exists(lang_dir):\n",
    "        os.makedirs(lang_dir)\n",
    "\n",
    "    # Path to save the zip file\n",
    "    zip_path = os.path.join(lang_dir, f\"{lang_code}.zip\")\n",
    "\n",
    "    # Download the zip file\n",
    "    if not os.path.exists(zip_path):\n",
    "        response = requests.get(url, stream=True)\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        with open(zip_path, 'wb') as f:\n",
    "            for data in tqdm(response.iter_content(1024), total=total_size // 1024, unit='KB'):\n",
    "                f.write(data)\n",
    "\n",
    "    # Extract the zip file\n",
    "    with ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(lang_dir)\n",
    "\n",
    "    # Remove the zip file after extraction\n",
    "    os.remove(zip_path)\n",
    "\n",
    "# Function to download the language data for the specified languages\n",
    "def download_languages(languages):\n",
    "    # Get the URLs of all language zip files\n",
    "    url_list = requests.get(base_url + \"zip_urls.txt\").text.splitlines()\n",
    "\n",
    "    # Filter the URLs for the specified languages\n",
    "    selected_urls = [url for url in url_list if any(lang in url for lang in languages)]\n",
    "\n",
    "    # Download and extract the selected languages in parallel\n",
    "    with Pool(4) as p:\n",
    "        p.map(download_and_extract, selected_urls)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: list of language codes to download (e.g., \"en\" for English, \"fr\" for French)\n",
    "    languages_to_download = [\"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", \"ru\", \"tr\"]  # Add or modify the languages here\n",
    "\n",
    "    download_languages(languages_to_download)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "# %pip install tensorflow, numpy, librosa, numpy\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D, Resizing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.image import resize\n",
    "from tensorflow.keras.models import load_model\n",
    "import tqdm\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varibles for the model\n",
    "batch_size = 32\n",
    "epochs = 80\n",
    "target_shape = (512, 512)\n",
    "max_files_per_class = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['de', 'en', 'es', 'fr', 'it', 'pt', 'ru', 'tr']\n",
      "Loading and preprocessing data with TensorFlow...\n",
      "Processing class de (1/8)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 3133/12864 [01:06<03:26, 47.10it/s]"
     ]
    }
   ],
   "source": [
    "# Function to convert waveform to spectrogram\n",
    "def get_spectrogram(waveform):\n",
    "    # Convert the waveform to a spectrogram using Short-Time Fourier Transform (STFT)\n",
    "    spectrogram = tf.signal.stft(waveform, frame_length=255, frame_step=128)\n",
    "    # Obtain the magnitude of the STFT\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    # Add a `channels` dimension so that the spectrogram can be used as image-like input data\n",
    "    spectrogram = spectrogram[..., tf.newaxis]\n",
    "    return spectrogram\n",
    "\n",
    "# Function to load and preprocess data using TensorFlow\n",
    "def load_and_preprocess_data_tf(data_dir, classes, target_shape=(128, 128), max_files_per_class=100):\n",
    "    print('Loading and preprocessing data with TensorFlow...')\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for i, class_name in enumerate(classes):\n",
    "        print(f'Processing class {class_name} ({i+1}/{len(classes)})...')\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        file_count = 0\n",
    "        for filename in tqdm.tqdm(os.listdir(class_dir)):\n",
    "            if filename.endswith('.wav'):\n",
    "                file_path = os.path.join(class_dir, filename)\n",
    "                # Load audio using TensorFlow\n",
    "                audio_binary = tf.io.read_file(file_path)\n",
    "                audio, sample_rate = tf.audio.decode_wav(audio_binary)\n",
    "                waveform = tf.squeeze(audio, axis=-1)\n",
    "                \n",
    "                # Convert to spectrogram\n",
    "                spectrogram = get_spectrogram(waveform)\n",
    "                spectrogram = tf.image.resize(spectrogram, target_shape)\n",
    "\n",
    "                data.append(spectrogram.numpy())\n",
    "                labels.append(i)\n",
    "                file_count += 1\n",
    "                if file_count >= max_files_per_class:\n",
    "                    break\n",
    "    print('Data loading and preprocessing complete.')\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Define your folder structure\n",
    "data_dir = 'data'\n",
    "classes = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "print('Classes:', classes)\n",
    "\n",
    "# Load and preprocess the data with TensorFlow\n",
    "data, labels = load_and_preprocess_data_tf(\n",
    "    data_dir, classes, target_shape=target_shape, max_files_per_class=max_files_per_class)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "labels = to_categorical(labels, num_classes=len(classes))\n",
    "\n",
    "# Create train_dataset and test_dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Loaded data shape:', X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some random samples\n",
    "\n",
    "def plot_spectrogram(spectrogram, ax):\n",
    "    if len(spectrogram.shape) > 2:\n",
    "        assert len(spectrogram.shape) == 3\n",
    "        spectrogram = np.squeeze(spectrogram, axis=-1)\n",
    "    # Convert the frequencies to log scale and transpose, so that the time is\n",
    "    # represented on the x-axis (columns).\n",
    "    # Add an epsilon to avoid taking a log of zero.\n",
    "    log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
    "    height = log_spec.shape[0]\n",
    "    width = log_spec.shape[1]\n",
    "    X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
    "    Y = range(height)\n",
    "    ax.pcolormesh(X, Y, log_spec)\n",
    "\n",
    "\n",
    "def show_random_samples(data, labels, classes, num_samples=5):\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(20, 4))\n",
    "    indices = np.random.choice(len(data), num_samples, replace=False)\n",
    "    for i, ax in enumerate(axes):\n",
    "        idx = indices[i]\n",
    "        spectrogram = data[idx]\n",
    "        label = classes[np.argmax(labels[idx])]\n",
    "        plot_spectrogram(spectrogram, ax)\n",
    "        ax.set_title(label)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Show random samples from the test set\n",
    "show_random_samples(X_test, y_test, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset cache and prefetch\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.cache()\n",
    "test_dataset = test_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train[0].shape\n",
    "\n",
    "# Normalize the data\n",
    "norm_layer = tf.keras.layers.Normalization()\n",
    "norm_layer.adapt(X_train)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Input(shape=input_shape),\n",
    "    # Downsample the input.\n",
    "    Resizing(32, 32),\n",
    "    # Normalize.\n",
    "    norm_layer,\n",
    "    Conv2D(32, 3, activation='relu'),\n",
    "    Conv2D(64, 3, activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(classes), activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingByAccuracyDiff(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, threshold=0.2):\n",
    "        super(EarlyStoppingByAccuracyDiff, self).__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        train_acc = logs.get('accuracy')\n",
    "        val_acc = logs.get('val_accuracy')\n",
    "\n",
    "        if train_acc is not None and val_acc is not None:\n",
    "            if (train_acc - val_acc) > self.threshold:\n",
    "                print(f\"\\nStopping training: train_acc={train_acc}, val_acc={val_acc}, diff={train_acc - val_acc}\")\n",
    "                self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoint callback to save the latest model\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='latest_model.keras',  # Use the .keras extension\n",
    "    save_best_only=True,  # Save the best model only at the end of each epoch\n",
    "    save_weights_only=False,  # Save the entire model\n",
    "    monitor='val_accuracy',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('latest_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStoppingByAccuracyDiff(threshold=0.2)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=[tensorboard_callback, checkpoint_callback, early_stop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy=model.evaluate(X_test,y_test,verbose=0)\n",
    "print(test_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('audio_classification_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model prediction on audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model = load_model('audio_classification_model.keras')\n",
    "\n",
    "target_shape = (128, 128)\n",
    "\n",
    "# Function to preprocess and classify an audio file\n",
    "def test_audio(file_path, model):\n",
    "    # Load and preprocess the audio file\n",
    "    audio_binary = tf.io.read_file(file_path)\n",
    "    audio, sample_rate = tf.audio.decode_wav(audio_binary)\n",
    "    waveform = tf.squeeze(audio, axis=-1)\n",
    "    mel_spectrogram = get_spectrogram(waveform)\n",
    "    mel_spectrogram = tf.image.resize(mel_spectrogram, target_shape)\n",
    "    mel_spectrogram = mel_spectrogram.numpy()\n",
    "    mel_spectrogram = mel_spectrogram[np.newaxis, ...]\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(mel_spectrogram)\n",
    "    \n",
    "    # Get the class probabilities\n",
    "    class_probabilities = predictions[0]\n",
    "    \n",
    "    # Get the predicted class index\n",
    "    predicted_class_index = np.argmax(class_probabilities)\n",
    "    \n",
    "    return class_probabilities, predicted_class_index\n",
    "\n",
    "# Test an audio file\n",
    "test_audio_file = \"data/es/yqUcDCVhytk__U__S11---0082.030-0097.400.wav\"\n",
    "class_probabilities, predicted_class_index = test_audio(test_audio_file, model)\n",
    "\n",
    "# Display results for all classes\n",
    "for i, class_label in enumerate(classes):\n",
    "    probability = class_probabilities[i]\n",
    "    print(f'Class: {class_label}, Probability: {probability:.4f}')\n",
    "\n",
    "# Calculate and display the predicted class and accuracy\n",
    "predicted_class = classes[predicted_class_index]\n",
    "accuracy = class_probabilities[predicted_class_index]\n",
    "print(f'The audio is classified as: {predicted_class}')\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
